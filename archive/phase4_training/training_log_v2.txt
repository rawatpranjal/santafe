üìã Loading configuration: ppo_vs_zic

============================================================
üéØ PPO TRAINING: ppo_vs_zic
üìù Train PPO agent against ZIC (Zero-Intelligence Constrained) opponents
============================================================
Environment: 8 agents, 4 tokens
Opponents: ZIC
Total Timesteps: 500,000
Parallel Envs: 2
============================================================

üèóÔ∏è Creating environments...
ü§ñ Creating new PPO model...
Using cpu device

üöÄ Starting training...
  Total timesteps: 500,000
  Learning rate: 0.0005
  Batch size: 64
  Entropy coefficient: 0.03
------------------------------------------------------------

Eval num_timesteps=20000, episode_reward=5.27 +/- 0.91
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.27        |
| time/                   |             |
|    total_timesteps      | 20000       |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 37          |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.014495125 |
|    clip_fraction        | 0.257       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.392       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.016       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.152       |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=5.34 +/- 1.26
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.34        |
| time/                   |             |
|    total_timesteps      | 40000       |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 13          |
|    profit               | 2           |
|    profitable_trades    | 1           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.012876147 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.31       |
|    explained_variance   | 0.88        |
|    learning_rate        | 0.0005      |
|    loss                 | -0.011      |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 0.0536      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=60000, episode_reward=4.96 +/- 1.03
Episode length: 99.20 +/- 5.60
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.2         |
|    mean_reward          | 4.96         |
| time/                   |              |
|    total_timesteps      | 60000        |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 5            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 0            |
| train/                  |              |
|    approx_kl            | 0.0091123525 |
|    clip_fraction        | 0.134        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.11        |
|    explained_variance   | 0.757        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.00891      |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.00502     |
|    value_loss           | 0.184        |
------------------------------------------
Eval num_timesteps=80000, episode_reward=4.68 +/- 1.92
Episode length: 99.18 +/- 5.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.2        |
|    mean_reward          | 4.68        |
| time/                   |             |
|    total_timesteps      | 80000       |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 5           |
|    profit               | -4          |
|    profitable_trades    | 0           |
|    trades               | 3           |
| train/                  |             |
|    approx_kl            | 0.019019317 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.02       |
|    explained_variance   | 0.894       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0275     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.00449    |
|    value_loss           | 0.0774      |
-----------------------------------------
Eval num_timesteps=100000, episode_reward=4.63 +/- 2.04
Episode length: 99.86 +/- 0.98
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.9       |
|    mean_reward          | 4.63       |
| time/                   |            |
|    total_timesteps      | 100000     |
| trading/                |            |
|    efficiency           | 0          |
|    invalid_actions      | 4          |
|    profit               | -2         |
|    profitable_trades    | 0          |
|    trades               | 1          |
| train/                  |            |
|    approx_kl            | 0.01005118 |
|    clip_fraction        | 0.0776     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.967     |
|    explained_variance   | 0.906      |
|    learning_rate        | 0.0005     |
|    loss                 | 0.0227     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.00113   |
|    value_loss           | 0.0681     |
----------------------------------------
Eval num_timesteps=120000, episode_reward=4.81 +/- 0.96
Episode length: 99.38 +/- 3.04
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 99.4         |
|    mean_reward          | 4.81         |
| time/                   |              |
|    total_timesteps      | 120000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 3            |
|    profit               | 3            |
|    profitable_trades    | 1            |
|    trades               | 1            |
| train/                  |              |
|    approx_kl            | 0.0051548555 |
|    clip_fraction        | 0.0671       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.991       |
|    explained_variance   | 0.78         |
|    learning_rate        | 0.0005       |
|    loss                 | 0.0529       |
|    n_updates            | 290          |
|    policy_gradient_loss | -0.00204     |
|    value_loss           | 0.204        |
------------------------------------------
Eval num_timesteps=140000, episode_reward=5.15 +/- 0.89
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.15        |
| time/                   |             |
|    total_timesteps      | 140000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 5           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.011607238 |
|    clip_fraction        | 0.0918      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.17       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0373      |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00132    |
|    value_loss           | 0.182       |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=5.13 +/- 0.64
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.13        |
| time/                   |             |
|    total_timesteps      | 160000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 18          |
|    profit               | 2           |
|    profitable_trades    | 1           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.010056522 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1          |
|    explained_variance   | 0.882       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0117      |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00492    |
|    value_loss           | 0.0902      |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=5.15 +/- 0.65
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.15        |
| time/                   |             |
|    total_timesteps      | 180000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 11          |
|    profit               | 3           |
|    profitable_trades    | 2           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.009386959 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.955      |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0312     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.00344    |
|    value_loss           | 0.152       |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=5.44 +/- 1.06
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.44        |
| time/                   |             |
|    total_timesteps      | 200000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 3           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.008356647 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.958      |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0623      |
|    n_updates            | 480         |
|    policy_gradient_loss | -0.00476    |
|    value_loss           | 0.0964      |
-----------------------------------------
New best mean reward!
Eval num_timesteps=220000, episode_reward=5.46 +/- 1.08
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 5.46         |
| time/                   |              |
|    total_timesteps      | 220000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 1            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 1            |
| train/                  |              |
|    approx_kl            | 0.0075276284 |
|    clip_fraction        | 0.0927       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.882       |
|    explained_variance   | 0.854        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.000297     |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00522     |
|    value_loss           | 0.111        |
------------------------------------------
New best mean reward!
Eval num_timesteps=240000, episode_reward=5.31 +/- 0.86
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.31        |
| time/                   |             |
|    total_timesteps      | 240000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 10          |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.006121322 |
|    clip_fraction        | 0.0664      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.871      |
|    explained_variance   | 0.917       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0198     |
|    n_updates            | 580         |
|    policy_gradient_loss | -0.00253    |
|    value_loss           | 0.0835      |
-----------------------------------------
Eval num_timesteps=260000, episode_reward=5.38 +/- 0.85
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 5.38         |
| time/                   |              |
|    total_timesteps      | 260000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 2            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 0            |
| train/                  |              |
|    approx_kl            | 0.0031089922 |
|    clip_fraction        | 0.0418       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.708       |
|    explained_variance   | 0.897        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.000697     |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 0.0903       |
------------------------------------------
Eval num_timesteps=280000, episode_reward=5.27 +/- 0.85
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.27        |
| time/                   |             |
|    total_timesteps      | 280000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 3           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.008163733 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.834      |
|    explained_variance   | 0.89        |
|    learning_rate        | 0.0005      |
|    loss                 | 0.111       |
|    n_updates            | 680         |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 0.0957      |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=5.17 +/- 0.48
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.17        |
| time/                   |             |
|    total_timesteps      | 300000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 25          |
|    profit               | 8           |
|    profitable_trades    | 2           |
|    trades               | 3           |
| train/                  |             |
|    approx_kl            | 0.022937648 |
|    clip_fraction        | 0.0648      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.75       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0574      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00258    |
|    value_loss           | 0.201       |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=5.48 +/- 0.98
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.48        |
| time/                   |             |
|    total_timesteps      | 320000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 2           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.007719526 |
|    clip_fraction        | 0.0529      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.724      |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.00856     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00146    |
|    value_loss           | 0.13        |
-----------------------------------------
New best mean reward!
Eval num_timesteps=340000, episode_reward=5.38 +/- 1.34
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.38        |
| time/                   |             |
|    total_timesteps      | 340000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 11          |
|    profit               | -1          |
|    profitable_trades    | 0           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.019728258 |
|    clip_fraction        | 0.0985      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.892      |
|    explained_variance   | 0.892       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0497      |
|    n_updates            | 830         |
|    policy_gradient_loss | -0.00384    |
|    value_loss           | 0.117       |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=5.35 +/- 0.91
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 5.35         |
| time/                   |              |
|    total_timesteps      | 360000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 2            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 0            |
| train/                  |              |
|    approx_kl            | 0.0072305608 |
|    clip_fraction        | 0.0874       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.797       |
|    explained_variance   | 0.871        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.000284     |
|    n_updates            | 870          |
|    policy_gradient_loss | -0.00315     |
|    value_loss           | 0.113        |
------------------------------------------
Eval num_timesteps=380000, episode_reward=5.31 +/- 0.93
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.31        |
| time/                   |             |
|    total_timesteps      | 380000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 3           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.013741212 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.951      |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.00471    |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00553    |
|    value_loss           | 0.109       |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=5.34 +/- 1.24
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.34        |
| time/                   |             |
|    total_timesteps      | 400000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 2           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.016729679 |
|    clip_fraction        | 0.0842      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.801      |
|    explained_variance   | 0.946       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0153     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00436    |
|    value_loss           | 0.0385      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 4.99        |
| time/                   |             |
|    fps                  | 2894        |
|    iterations           | 100         |
|    time_elapsed         | 141         |
|    total_timesteps      | 409600      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 1           |
|    profit               | -1          |
|    profitable_trades    | 0           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.016849067 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.849      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.0132      |
|    n_updates            | 990         |
|    policy_gradient_loss | -0.00595    |
|    value_loss           | 0.0959      |
-----------------------------------------
Eval num_timesteps=420000, episode_reward=5.23 +/- 0.85
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.23        |
| time/                   |             |
|    total_timesteps      | 420000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 3           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.010251354 |
|    clip_fraction        | 0.0909      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.832      |
|    explained_variance   | 0.859       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0164     |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00347    |
|    value_loss           | 0.124       |
-----------------------------------------
Eval num_timesteps=440000, episode_reward=5.23 +/- 0.90
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.23        |
| time/                   |             |
|    total_timesteps      | 440000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 1           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.013232874 |
|    clip_fraction        | 0.0993      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.887      |
|    explained_variance   | 0.849       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0115     |
|    n_updates            | 1070        |
|    policy_gradient_loss | -0.00489    |
|    value_loss           | 0.12        |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=4.96 +/- 0.28
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 4.96        |
| time/                   |             |
|    total_timesteps      | 460000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 25          |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.009163102 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.905      |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0005      |
|    loss                 | 0.102       |
|    n_updates            | 1120        |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 0.266       |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=5.12 +/- 0.67
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 5.12         |
| time/                   |              |
|    total_timesteps      | 480000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 7            |
|    profit               | 1            |
|    profitable_trades    | 1            |
|    trades               | 1            |
| train/                  |              |
|    approx_kl            | 0.0070577106 |
|    clip_fraction        | 0.0706       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.791       |
|    explained_variance   | 0.888        |
|    learning_rate        | 0.0005       |
|    loss                 | 0.017        |
|    n_updates            | 1170         |
|    policy_gradient_loss | -0.00281     |
|    value_loss           | 0.0969       |
------------------------------------------
Eval num_timesteps=500000, episode_reward=5.38 +/- 1.07
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 5.38        |
| time/                   |             |
|    total_timesteps      | 500000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 9           |
|    profit               | 4           |
|    profitable_trades    | 1           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.009707942 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.93       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0005      |
|    loss                 | -0.0152     |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.00785    |
|    value_loss           | 0.0886      |
-----------------------------------------

üíæ Saving final model...
  Model saved to models/ppo_vs_zic_v2/final_model.zip

üìä Final Evaluation...
Traceback (most recent call last):
  File "/Users/pranjal/Code/santafe-1/scripts/train_ppo_enhanced.py", line 514, in <module>
    main()
    ~~~~^^
  File "/Users/pranjal/Code/santafe-1/scripts/train_ppo_enhanced.py", line 471, in main
    results = evaluate_final_model(model, eval_env, n_episodes=100)
  File "/Users/pranjal/Code/santafe-1/scripts/train_ppo_enhanced.py", line 324, in evaluate_final_model
    if done:
       ^^^^
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
