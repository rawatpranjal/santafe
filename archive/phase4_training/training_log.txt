üìã Loading configuration: ppo_vs_zic

============================================================
üéØ PPO TRAINING: ppo_vs_zic
üìù Train PPO agent against ZIC (Zero-Intelligence Constrained) opponents
============================================================
Environment: 8 agents, 4 tokens
Opponents: ZIC
Total Timesteps: 500,000
Parallel Envs: 2
============================================================

üèóÔ∏è Creating environments...
ü§ñ Creating new PPO model...
Using cpu device

üöÄ Starting training...
  Total timesteps: 500,000
  Learning rate: 0.0003
  Batch size: 64
  Entropy coefficient: 0.01
------------------------------------------------------------

Eval num_timesteps=20000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0104      |
| time/                   |             |
|    total_timesteps      | 20000       |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 43          |
|    profit               | 1           |
|    profitable_trades    | 1           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.014168538 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.87       |
|    explained_variance   | 0.393       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.036      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00761    |
|    value_loss           | 5.75e-05    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=40000, episode_reward=0.01 +/- 0.02
Episode length: 98.70 +/- 6.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 98.7       |
|    mean_reward          | 0.00837    |
| time/                   |            |
|    total_timesteps      | 40000      |
| trading/                |            |
|    efficiency           | 0          |
|    invalid_actions      | 37         |
|    profit               | 0          |
|    profitable_trades    | 0          |
|    trades               | 1          |
| train/                  |            |
|    approx_kl            | 0.01694002 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.54      |
|    explained_variance   | 0.0454     |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0182    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 6.45e-05   |
----------------------------------------
Eval num_timesteps=60000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0113      |
| time/                   |             |
|    total_timesteps      | 60000       |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 8           |
|    profit               | 5           |
|    profitable_trades    | 1           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.010590633 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.21       |
|    explained_variance   | 0.225       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0337     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.00486    |
|    value_loss           | 1.4e-05     |
-----------------------------------------
New best mean reward!
Eval num_timesteps=80000, episode_reward=0.01 +/- 0.01
Episode length: 99.20 +/- 3.42
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 99.2       |
|    mean_reward          | 0.0104     |
| time/                   |            |
|    total_timesteps      | 80000      |
| trading/                |            |
|    efficiency           | 0          |
|    invalid_actions      | 1          |
|    profit               | 0          |
|    profitable_trades    | 0          |
|    trades               | 0          |
| train/                  |            |
|    approx_kl            | 0.01219159 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.03      |
|    explained_variance   | 0.489      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0179    |
|    n_updates            | 190        |
|    policy_gradient_loss | -0.00355   |
|    value_loss           | 3.36e-05   |
----------------------------------------
Eval num_timesteps=100000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0105      |
| time/                   |             |
|    total_timesteps      | 100000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 3           |
|    profit               | 1           |
|    profitable_trades    | 1           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.007856214 |
|    clip_fraction        | 0.0496      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.99       |
|    explained_variance   | 0.382       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0182     |
|    n_updates            | 240         |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 4.81e-05    |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=0.01 +/- 0.02
Episode length: 99.58 +/- 1.96
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 99.6        |
|    mean_reward          | 0.00945     |
| time/                   |             |
|    total_timesteps      | 120000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 3           |
|    profit               | -1          |
|    profitable_trades    | 0           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.007662493 |
|    clip_fraction        | 0.0445      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.957      |
|    explained_variance   | 0.339       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.000967   |
|    value_loss           | 3.21e-05    |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.011       |
| time/                   |             |
|    total_timesteps      | 140000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 1           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.013824077 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.894      |
|    explained_variance   | 0.403       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0125     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 2.36e-05    |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0126      |
| time/                   |             |
|    total_timesteps      | 160000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.008214084 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.871      |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00913    |
|    n_updates            | 390         |
|    policy_gradient_loss | -0.00252    |
|    value_loss           | 2.11e-05    |
-----------------------------------------
New best mean reward!
Eval num_timesteps=180000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.0124       |
| time/                   |              |
|    total_timesteps      | 180000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 0            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 0            |
| train/                  |              |
|    approx_kl            | 0.0076439884 |
|    clip_fraction        | 0.073        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.845       |
|    explained_variance   | 0.405        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00929     |
|    n_updates            | 430          |
|    policy_gradient_loss | -0.00241     |
|    value_loss           | 9.37e-06     |
------------------------------------------
Eval num_timesteps=200000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 100        |
|    mean_reward          | 0.0116     |
| time/                   |            |
|    total_timesteps      | 200000     |
| trading/                |            |
|    efficiency           | 0          |
|    invalid_actions      | 0          |
|    profit               | 0          |
|    profitable_trades    | 0          |
|    trades               | 0          |
| train/                  |            |
|    approx_kl            | 0.00697665 |
|    clip_fraction        | 0.0728     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.812     |
|    explained_variance   | 0.304      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0117    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.000962  |
|    value_loss           | 2.1e-05    |
----------------------------------------
Eval num_timesteps=220000, episode_reward=0.02 +/- 0.01
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.0161       |
| time/                   |              |
|    total_timesteps      | 220000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 0            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 1            |
| train/                  |              |
|    approx_kl            | 0.0079973955 |
|    clip_fraction        | 0.055        |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.717       |
|    explained_variance   | 0.271        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0468      |
|    n_updates            | 530          |
|    policy_gradient_loss | -0.00113     |
|    value_loss           | 1.9e-05      |
------------------------------------------
New best mean reward!
Eval num_timesteps=240000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.0145       |
| time/                   |              |
|    total_timesteps      | 240000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 1            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 1            |
| train/                  |              |
|    approx_kl            | 0.0033948335 |
|    clip_fraction        | 0.0438       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.439       |
|    explained_variance   | 0.242        |
|    learning_rate        | 0.0003       |
|    loss                 | 0.00554      |
|    n_updates            | 580          |
|    policy_gradient_loss | -0.000985    |
|    value_loss           | 2.05e-05     |
------------------------------------------
Eval num_timesteps=260000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.0135       |
| time/                   |              |
|    total_timesteps      | 260000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 6            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 0            |
| train/                  |              |
|    approx_kl            | 0.0132777225 |
|    clip_fraction        | 0.0942       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.577       |
|    explained_variance   | -0.0935      |
|    learning_rate        | 0.0003       |
|    loss                 | -0.0204      |
|    n_updates            | 630          |
|    policy_gradient_loss | -0.00573     |
|    value_loss           | 1.86e-05     |
------------------------------------------
Eval num_timesteps=280000, episode_reward=0.02 +/- 0.02
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.0193       |
| time/                   |              |
|    total_timesteps      | 280000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 0            |
|    profit               | 4            |
|    profitable_trades    | 1            |
|    trades               | 1            |
| train/                  |              |
|    approx_kl            | 0.0035422333 |
|    clip_fraction        | 0.0303       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.295       |
|    explained_variance   | 0.435        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00218     |
|    n_updates            | 680          |
|    policy_gradient_loss | -0.00159     |
|    value_loss           | 1.25e-05     |
------------------------------------------
New best mean reward!
Eval num_timesteps=300000, episode_reward=0.00 +/- 0.02
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.00436     |
| time/                   |             |
|    total_timesteps      | 300000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 1           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.012188181 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.459      |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0699      |
|    n_updates            | 730         |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 1.23e-05    |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0107      |
| time/                   |             |
|    total_timesteps      | 320000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.012655728 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.668      |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0262     |
|    n_updates            | 780         |
|    policy_gradient_loss | -0.00499    |
|    value_loss           | 1.98e-05    |
-----------------------------------------
Eval num_timesteps=340000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 100          |
|    mean_reward          | 0.0137       |
| time/                   |              |
|    total_timesteps      | 340000       |
| trading/                |              |
|    efficiency           | 0            |
|    invalid_actions      | 0            |
|    profit               | 0            |
|    profitable_trades    | 0            |
|    trades               | 0            |
| train/                  |              |
|    approx_kl            | 0.0053666374 |
|    clip_fraction        | 0.0588       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.647       |
|    explained_variance   | 0.571        |
|    learning_rate        | 0.0003       |
|    loss                 | -0.00929     |
|    n_updates            | 830          |
|    policy_gradient_loss | -0.00303     |
|    value_loss           | 1.07e-05     |
------------------------------------------
Eval num_timesteps=360000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0144      |
| time/                   |             |
|    total_timesteps      | 360000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.009551536 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.558      |
|    explained_variance   | 0.493       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.013      |
|    n_updates            | 870         |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 8.31e-06    |
-----------------------------------------
Eval num_timesteps=380000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0127      |
| time/                   |             |
|    total_timesteps      | 380000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.005936303 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.458      |
|    explained_variance   | 0.208       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0019     |
|    n_updates            | 920         |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 1.83e-05    |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=0.01 +/- 0.02
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.00573     |
| time/                   |             |
|    total_timesteps      | 400000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 1           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.009391644 |
|    clip_fraction        | 0.0788      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.445      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0136     |
|    n_updates            | 970         |
|    policy_gradient_loss | -0.00136    |
|    value_loss           | 9.61e-06    |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 100         |
|    ep_rew_mean          | 0.00526     |
| time/                   |             |
|    fps                  | 2848        |
|    iterations           | 100         |
|    time_elapsed         | 143         |
|    total_timesteps      | 409600      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 2           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.018217284 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.504      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00488    |
|    n_updates            | 990         |
|    policy_gradient_loss | 0.0016      |
|    value_loss           | 1.46e-05    |
-----------------------------------------
Eval num_timesteps=420000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.00999     |
| time/                   |             |
|    total_timesteps      | 420000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.008585731 |
|    clip_fraction        | 0.0821      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.519      |
|    explained_variance   | 0.4         |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00352    |
|    n_updates            | 1020        |
|    policy_gradient_loss | -0.00329    |
|    value_loss           | 2.29e-05    |
-----------------------------------------
Eval num_timesteps=440000, episode_reward=0.01 +/- 0.00
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.00978     |
| time/                   |             |
|    total_timesteps      | 440000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 1           |
| train/                  |             |
|    approx_kl            | 0.003908447 |
|    clip_fraction        | 0.036       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.331      |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00748    |
|    n_updates            | 1070        |
|    policy_gradient_loss | 0.000147    |
|    value_loss           | 1.22e-05    |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 100        |
|    mean_reward          | 0.0126     |
| time/                   |            |
|    total_timesteps      | 460000     |
| trading/                |            |
|    efficiency           | 0          |
|    invalid_actions      | 2          |
|    profit               | 0          |
|    profitable_trades    | 0          |
|    trades               | 1          |
| train/                  |            |
|    approx_kl            | 0.00389349 |
|    clip_fraction        | 0.036      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.262     |
|    explained_variance   | 0.136      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0033    |
|    n_updates            | 1120       |
|    policy_gradient_loss | -0.00235   |
|    value_loss           | 1.32e-05   |
----------------------------------------
Eval num_timesteps=480000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0144      |
| time/                   |             |
|    total_timesteps      | 480000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 0           |
| train/                  |             |
|    approx_kl            | 0.007686074 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.339      |
|    explained_variance   | 0.221       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0172     |
|    n_updates            | 1170        |
|    policy_gradient_loss | 9.41e-05    |
|    value_loss           | 9.68e-06    |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=0.01 +/- 0.01
Episode length: 100.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 100         |
|    mean_reward          | 0.0145      |
| time/                   |             |
|    total_timesteps      | 500000      |
| trading/                |             |
|    efficiency           | 0           |
|    invalid_actions      | 0           |
|    profit               | 0           |
|    profitable_trades    | 0           |
|    trades               | 2           |
| train/                  |             |
|    approx_kl            | 0.020427998 |
|    clip_fraction        | 0.0585      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.338      |
|    explained_variance   | 0.359       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00862    |
|    n_updates            | 1220        |
|    policy_gradient_loss | -0.000889   |
|    value_loss           | 1.12e-05    |
-----------------------------------------

üíæ Saving final model...
  Model saved to models/ppo_vs_zic/final_model.zip

üìä Final Evaluation...
Traceback (most recent call last):
  File "/Users/pranjal/Code/santafe-1/scripts/train_ppo_enhanced.py", line 514, in <module>
    main()
    ~~~~^^
  File "/Users/pranjal/Code/santafe-1/scripts/train_ppo_enhanced.py", line 471, in main
    results = evaluate_final_model(model, eval_env, n_episodes=100)
  File "/Users/pranjal/Code/santafe-1/scripts/train_ppo_enhanced.py", line 324, in evaluate_final_model
    if done:
       ^^^^
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
