# @package _global_
#
# Experiment 2.1: PPO vs ZIC (Curriculum Level 1)
#
# Goal: Train PPO agent to exploit Zero-Intelligence Constrained (ZIC) agents
# by learning bid shading strategies.
#
# Expected outcome: PPO learns to bid slightly below valuation (buyers) or
# ask slightly above cost (sellers) to extract profit from ZIC's random quotes.

defaults:
  - /rl/ppo
  - /rl/vectorization
  - /rl/training
  - _self_

# Experiment metadata
experiment:
  name: "2_1_ppo_vs_zic"
  description: "PPO learns to trade against ZIC opponents (curriculum stage 1)"
  phase: "4.1"
  task: "Train PPO with ZIC opponents"

# Training configuration
training:
  total_timesteps: 1_000_000

  # W&B logging (Phase 4.1 requirement)
  wandb:
    enabled: true
    name: "ppo_vs_zic_curriculum_level_1"
    tags: ["ppo", "zic", "curriculum-stage-1", "phase-4.1"]
    notes: |
      Experiment 2.1: PPO learns basic trading against ZIC agents.
      Success metric: PPO profit > ZIC profit (exploitation test).

# Environment configuration (5 buyers + 5 sellers)
vectorization:
  env:
    num_buyers: 5
    num_sellers: 5
    num_tokens_per_agent: 3
    max_timesteps: 100
    price_min: 0
    price_max: 100
    rl_agent_type: "buyer"  # PPO is buyer
    opponent_type: "ZIC"    # All opponents are ZIC
    seed: 42

# PPO hyperparameters (use defaults from /rl/ppo.yaml)
# Learning rate, batch size, etc. can be overridden here if needed
