# @package _global_
#
# Ultra-Minimal Debug Config for LLM Testing
# Cost: ~10 API calls, ~$0.001 per run
#
# Use this config to quickly test:
# - Prompt/response logging
# - max_time fix validation
# - Invalid action logging
# - GPT decision quality
#
# Usage:
#   python scripts/run_llm_experiment.py --config debug

experiment:
  name: "debug_llm"
  num_rounds: 1
  log_level: "DEBUG"  # Verbose logging for debugging
  rng_seed_auction: 42
  rng_seed_values: 123

market:
  num_periods: 1
  num_steps: 5        # Only 5 steps (vs 20 in minimal_test, 100 in full)
  num_tokens: 2
  min_price: 0
  max_price: 1000
  gametype: 6453      # SFI game type (same as base config)

agents:
  buyer_types:
    - GPT4-mini  # Test GPT buyer
    - ZIC        # Baseline comparison
  seller_types:
    - GPT4-mini  # Test GPT seller
    - ZIC        # Baseline comparison
