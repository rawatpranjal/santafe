# PPO Hyperparameters for Double Auction Trading
#
# Based on Stable-Baselines3 defaults with adjustments for:
# - Discrete action space (4 actions)
# - Continuous observation space (9 dimensions, normalized to [0, 1])
# - Sparse rewards (only on trades)
# - Action masking support

# Policy architecture
policy: "MlpPolicy"  # Multi-Layer Perceptron policy
policy_kwargs:
  net_arch:
    # Larger network for market microstructure learning
    - 256  # Hidden layer 1
    - 256  # Hidden layer 2
  activation_fn: "torch.nn.ReLU"

# Learning rate schedule
learning_rate: 0.0001  # Lower LR for larger network stability
lr_schedule: "constant"  # Options: constant, linear

# PPO rollout parameters
n_steps: 4096  # Larger rollouts reduce variance
batch_size: 256  # Better gradient estimates
n_epochs: 20  # More policy refinement per rollout

# PPO clipping and losses
clip_range: 0.2  # PPO clip parameter
clip_range_vf: null  # Clip value function (null = no clipping)
ent_coef: 0.15  # Initial entropy (will decay to 0.005 via callback)
vf_coef: 0.5  # Value function coefficient
max_grad_norm: 0.5  # Gradient clipping

# Discount and GAE
gamma: 0.99  # Discount factor
gae_lambda: 0.95  # GAE lambda parameter

# Normalization
normalize_advantage: true  # Normalize advantages
use_sde: false  # State-dependent exploration (for continuous actions)

# Technical settings
verbose: 1  # Logging level (0: none, 1: info, 2: debug)
seed: 42  # Random seed for reproducibility
device: "auto"  # "cpu", "cuda", or "auto"

# Tensorboard logging
tensorboard_log: null  # Set to "./runs/" to enable if tensorboard installed
tb_log_name: "ppo_double_auction"
