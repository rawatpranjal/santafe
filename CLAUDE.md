Of course. Here is a HOW_TO_USE_FOR_CLAUDE.md file.

This document is designed to be a simple, clear guide for you (and other LLMs) to understand and interact with the repository. It explains the project's structure, how to perform the key tasks (testing, running experiments, analyzing results), and where to find the most important files. This ensures that you can efficiently get up to speed on the project's status and assist with development, analysis, or writing tasks.

code
Markdown
download
content_copy
expand_less

# How to Use This Repository (For Claude Assistant)

**Project:** `santafe-1` - A research platform for simulating the Santa Fe Double Auction market and studying trading agent behavior.

**Objective:** To replicate foundational economic experiments and test the performance of modern Reinforcement Learning agents against classical heuristics.

This document provides a guide to the repository's structure and the core workflows for testing, running experiments, and analyzing results.

---

## 1. Core Repository Structure

The project is organized into four main directories, each with a distinct purpose:

-   **`/src/santafe_gym/`**: This is the core simulation library. It contains all the fundamental Python code that makes the market run.
    -   `auction.py`: The market engine. Implements the rules of the double auction.
    -   `utils.py`: Helper functions for equilibrium calculation, data analysis, and plotting.
    -   `traders/`: Contains the "brains" of each trading agent. Each `.py` file defines a different strategy (e.g., `kp.py` for Kaplan, `zip.py` for ZIP, `ppo_handcrafted.py` for our main RL agent).

-   **`/tests/`**: The verification suite. Contains `pytest` tests that ensure the code in `src/` is correct and behaves as expected.

-   **`/experiments/`**: This is where we define our scientific studies.
    -   `configs/`: Contains Python dictionaries that specify the parameters for each experiment (e.g., which agents participate, how many rounds to run). This is the primary place to define new experiments.
    -   `run_experiments.py`: The main script for launching a simulation based on a given config file.

-   **`/analysis/`**: Scripts for processing the raw data generated by experiments.
    -   `generate_report.py`: The primary tool for "one-click" analysis. It loads the results of an experiment and generates a summary report with tables and figures.

-   **`/results/`**: **(Generated Directory)** This is where all output from experiments is saved. Each experiment run creates a subdirectory here containing logs, data (`.csv`), plots (`.png`), and the final analysis report (`.md`).

---

## 2. Core Workflows (How to Perform Key Tasks)

This project is designed around a "one-click" philosophy for key tasks.

### Task 1: Verify the Codebase is Working Correctly

**Objective:** Run the full test suite to ensure all components are functioning as expected before running long experiments.

**Command:** From the repository root directory, execute the test script:

```bash
./scripts/run_all_tests.sh

What it does: This script automatically finds and runs all pytest tests in the /tests/ directory. It verifies the auction mechanics, utility functions, and the logic of individual agents.

Expected Output: A message saying "âœ… All tests passed successfully!" and a code coverage report. If any tests fail, it will provide a detailed error report.

Task 2: Run a Scientific Experiment

Objective: Execute one of the predefined experimental phases to generate data for the paper.

Command: Use the experiments/run_experiments.py script, specifying which phase you want to run.

Example: To run the Phase 1 Classical Benchmark:

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
python experiments/run_experiments.py --phase 1

What it does: This script will find the configuration file for the Phase 1 benchmark (experiments/configs/phase1_classical_benchmark.py), launch the simulation, and run it to completion (this may take a long time). After the simulation is finished, it will automatically call the analysis script to generate a report.

To run Phase 2 (RL agent evaluation):

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
python experiments/run_experiments.py --phase 2

To run Phase 3 (MARL dynamics):

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
python experiments/run_experiments.py --phase 3
Task 3: Analyze the Results of an Experiment

Objective: Generate (or re-generate) a summary report from an existing experiment's data.

Command: Use the analysis/generate_report.py script, pointing it to the specific results directory.

Example: To re-analyze the Phase 1 benchmark results:

code
Bash
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END
python analysis/generate_report.py --exp_dir results/phase1_classical_benchmark/

What it does: This script loads the round_log_all.csv from the specified directory, runs all the analysis functions from utils.py (market performance, individual performance, tournament ranking), and writes a clean ANALYSIS_REPORT.md file back into that same directory.

3. How to Assist with this Project

To review an agent's logic: Look in src/santafe_gym/traders/. The ppo_handcrafted.py file is our most advanced agent.

To understand an experiment's setup: Look in experiments/configs/. The files are named according to the research phase (e.g., phase1_..., phase2_...).

To see the latest results: Look in the /results/ directory for the most recent experiment folder and open the ANALYSIS_REPORT.md file.

To add a new experiment: Create a new configuration file in experiments/configs/ and add its name to the appropriate list in experiments/run_experiments.py.

code
Code
download
content_copy
expand_less
IGNORE_WHEN_COPYING_START
IGNORE_WHEN_COPYING_END