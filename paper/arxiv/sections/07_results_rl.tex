\subsection{PPO Agent Design}

We train a deep reinforcement learning agent using Proximal Policy Optimization (PPO) with action masking (MaskablePPO) to ensure valid bids under AURORA protocol constraints. The agent uses a two-layer neural network with 256 hidden units per layer and receives a 42-dimensional observation including market state, order book information, and private token values. Training proceeds for 10 million timesteps against mixed opponents (ZIC, ZIP, Skeleton, GD, Kaplan) using entropy coefficient decay from 0.15 to 0.01 to encourage policy refinement.

A critical methodological finding concerns role specialization. When the PPO model trained as a buyer was deployed in both buyer and seller roles, performance degraded significantly. The model had never observed the seller perspective during training, leading to poor seller-side decision making. We therefore restrict PPO to buyer-only deployment in all experiments below, matching its training distribution.

\subsection{PPO vs Zero-Intelligence Baselines}

Section 5 established the zero-intelligence hierarchy: ZI fails catastrophically, ZIC achieves 97\% efficiency through the budget constraint alone, and ZIP achieves 99\% efficiency through adaptive margin learning. A natural question arises: where does deep reinforcement learning fall in this hierarchy? Can PPO exceed the hand-crafted adaptive learning of ZIP?

We train PPO specifically against ZIC and ZIP opponents for $10^6$ timesteps, then evaluate in a mixed market with one agent of each type per role: four buyers (ZI, ZIC, ZIP, PPO) and four sellers (ZI, ZIC, ZIP, ZIC). We run 50 rounds with 10 periods each across 10 random seeds for statistical robustness.

\begin{table}[H]
    \centering
    \caption{PPO vs Zero-Intelligence Baseline Tournament (10 seeds, 50 rounds each)}
    \label{tab:ppo_zi_tournament}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Strategy} & \textbf{Mean Profit} & \textbf{Std Dev} & \textbf{Mean Rank} \\
        \midrule
        \textbf{PPO} & \textbf{138,772} & \textbf{11,383} & \textbf{1.2} \\
        ZIP & 126,125 & 10,613 & 1.8 \\
        ZIC & 61,361 & 5,563 & 3.0 \\
        ZI & $-$165,285 & 26,002 & 4.0 \\
        \bottomrule
    \end{tabular}
\end{table}

PPO achieves mean profit of 138,772 with rank 1.2, outperforming ZIP (126,125, rank 1.8) by 10\%. ZIC places third with profit 61,361, while unconstrained ZI accumulates catastrophic losses. Deep RL can exceed hand-crafted adaptive heuristics: ZIP's momentum-based margin adjustment, designed through careful analysis of market dynamics, is surpassed by a neural network that discovers its own trading patterns through trial and error.

\subsubsection{Market-Level Effects}

Table \ref{tab:ppo_zi_market_metrics} compares allocative efficiency and price volatility across four market compositions: pure ZI, pure ZIC, pure ZIP, and the PPO+mix market.

\begin{table}[H]
    \centering
    \caption{Market Metrics by Composition (10 seeds, 50 rounds each)}
    \label{tab:ppo_zi_market_metrics}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Market Type} & \textbf{Efficiency} & \textbf{Volatility} & \textbf{V-Ineff} & \textbf{Trades/Period} \\
        \midrule
        ZI only     & 29.4\% & 69.7\% & 0.00 & 16.0 \\
        ZIC only    & 97.4\% &  7.8\% & 0.27 &  8.0 \\
        ZIP only    & 99.1\% & 11.2\% & 0.59 &  7.5 \\
        \textbf{PPO+mix} & \textbf{58.2\%} & \textbf{40.0\%} & \textbf{0.03} & \textbf{10.9} \\
        \bottomrule
    \end{tabular}
\end{table}

The PPO+mix market shows intermediate efficiency (58.2\%) between pure ZI (29.4\%) and pure ZIC/ZIP (97-99\%). This reflects the heterogeneous agent composition where PPO exploits ZI's random trading while ZIC and ZIP maintain some price discipline.

\subsubsection{Individual Strategy Profits}

Table \ref{tab:ppo_zi_profits} shows per-strategy profits when all four types compete simultaneously.

\begin{table}[H]
    \centering
    \caption{Profit by Strategy in PPO+mix Market}
    \label{tab:ppo_zi_profits}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Strategy} & \textbf{Mean Profit} & \textbf{Std Dev} \\
        \midrule
        ZIP & 2,831 & 149 \\
        PPO & 2,826 & 295 \\
        ZIC & 1,439 &  93 \\
        ZI  & $-$3,271 & 279 \\
        \bottomrule
    \end{tabular}
\end{table}

PPO and ZIP achieve nearly identical profits (2,826 vs 2,831) in the mixed market. Both strategies extract surplus from ZI's catastrophic losses while ZIC captures moderate profits.

Figure \ref{fig:ppo_zi_combined} presents a combined visualization of profit comparison, market efficiency, price volatility, and trading volume across the four market compositions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/ppo_zi_combined.pdf}
    \caption{PPO vs Zero-Intelligence analysis. (a) Profit by strategy in the mixed market shows PPO and ZIP achieving equivalent profits. (b) Market efficiency comparison across compositions. (c) Price volatility by market type. (d) Trading volume per period. PPO+mix (red) shows intermediate characteristics between pure ZI and ZIC/ZIP markets.}
    \label{fig:ppo_zi_combined}
\end{figure}

\subsection{Against Control}

Following the experimental framework established for legacy strategies in Section 6, we first evaluate PPO against a control population of 7 ZIC agents across all 10 tournament environments. The market composition is 1 PPO buyer, 3 ZIC buyers, and 4 ZIC sellers.

\input{figures/table_ppo_control.tex}

Table \ref{tab:ppo_control} shows market efficiency when PPO enters ZIC-dominated markets. In favorable-spread environments (BASE, BBBS, BSSS, PER, TOK), PPO maintains efficiency above 95\%, comparable to legacy strategy performance. However, efficiency degrades substantially in challenging environments: EQL (28\%), RAN (24\%), and SML (36\%). The SHRT environment shows moderate degradation (79\%) due to reduced trading time, while LAD exhibits high variance (57\% with std 37\%) reflecting sensitivity to extreme token values.

\input{figures/table_ppo_invasibility.tex}

Table \ref{tab:ppo_invasibility} presents profit ratios measuring PPO's ability to exploit ZIC populations. PPO demonstrates invasibility greater than 1.0x in most environments: BASE (1.27x), BBBS (1.41x), BSSS (1.09x), PER (1.26x), SHRT (1.58x), and TOK (1.34x). Notably, PPO struggles in environments with compressed profit margins (EQL 0.31x, SML 0.32x), where the budget constraint provides ZIC with a natural advantage. The LAD environment shows extreme invasibility (506x) due to massive profit differentials when buyers have high-value tokens.

\subsection{Pairwise Competition}

We evaluate PPO in mixed markets against individual legacy strategies, with 2 PPO buyers and 2 opponent buyers competing against 4 opponent sellers. This configuration tests PPO's ability to compete directly against sophisticated strategies in the BASE environment.

\input{figures/table_ppo_pairwise.tex}

Table \ref{tab:ppo_pairwise} presents pairwise competition results. PPO achieves positive profit ratios against all four opponents tested: ZIC (1.10x), demonstrating modest advantage over the zero-intelligence baseline; ZIP, showing PPO can compete with adaptive margin learners; Skeleton, the original Santa Fe champion; and Kaplan, the strategic sniper. Market efficiency remains high (above 95\%) in all pairwise configurations, indicating that PPO's profit extraction does not significantly degrade allocative outcomes.

\subsection{Round-Robin Tournament}

To provide a comprehensive evaluation, we conduct a 9-strategy round-robin tournament including all legacy strategies from Section 6: ZIC, ZIP, GD, Kaplan, Ringuette, Skeleton, EL, and Markup. PPO uses the 8 million step checkpoint from training.

\begin{table}[H]
    \centering
    \caption{Extended Tournament Results (9 Strategies, PPO Buyer-Only, 8M Training Steps)}
    \label{tab:ppo_extended_tournament}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Strategy} & \textbf{Mean Profit} & \textbf{Std Dev} & \textbf{Rank} \\
        \midrule
        \textbf{PPO (8M)} & \textbf{1404.2} & \textbf{715.6} & \textbf{1} \\
        Ringuette & 1384.2 & 585.8 & 2 \\
        EL & 1251.5 & 808.3 & 3 \\
        GD & 1184.6 & 631.5 & 4 \\
        Markup & 1131.4 & 607.1 & 5 \\
        Skeleton & 1124.7 & 648.1 & 6 \\
        Kaplan & 1119.3 & 688.7 & 7 \\
        ZIC & 891.3 & 426.7 & 8 \\
        ZIP & 863.2 & 534.5 & 9 \\
        \bottomrule
    \end{tabular}
\end{table}

At 8 million training steps, PPO achieves first place with mean profit 1404.2, surpassing Ringuette (1384.2) by 1.4\%. This represents a significant result: deep reinforcement learning has discovered a trading strategy that outperforms all hand-crafted heuristics developed over three decades of double auction research. PPO surpasses Easley-Ledyard (EL), the Kaplan sniper, Skeleton, GD, and the previously dominant Ringuette algorithm.

Figure \ref{fig:ppo_tournament_bar} visualizes the strategy hierarchy, with PPO in red placing first. The error bars indicate substantial variance in individual trader profits, characteristic of market competition where outcomes depend heavily on counterparty behavior.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ppo_tournament_bar.pdf}
    \caption{Extended tournament results showing mean profit with standard deviation error bars. PPO (red) achieves first place, surpassing all eight hand-crafted legacy strategies including Ringuette.}
    \label{fig:ppo_tournament_bar}
\end{figure}

The hierarchy revealed by this extended tournament differs from simpler evaluations. PPO emerges as the dominant strategy, while ZIP unexpectedly ranks last despite strong performance against zero-intelligence agents. This suggests that adaptive margin adjustment (ZIP) is less effective against sophisticated opponents than against simple benchmarks.

\subsection{Learning Dynamics}

Figure \ref{fig:ppo_learning_curve} shows the PPO learning curve with horizontal reference lines for legacy strategy baselines. The trajectory exhibits high variance characteristic of competitive multi-agent environments where small policy changes can produce significantly different market outcomes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/ppo_learning_curve.pdf}
    \caption{PPO learning curve showing evaluation reward versus training steps. Horizontal lines indicate legacy strategy baselines. PPO surpasses all legacy strategies including Ringuette by 8M steps.}
    \label{fig:ppo_learning_curve}
\end{figure}

\subsection{PPO Trading Behavior}

To understand the mechanism behind PPO's success, we analyze its bidding patterns across 25 trading periods (5 seeds, 5 periods each). Table \ref{tab:ppo_behavior} summarizes the temporal and strategic characteristics of PPO's trading decisions.

\begin{table}[H]
    \centering
    \caption{PPO Trading Behavior Patterns (5 seeds, 5 periods each, v10 8M checkpoint)}
    \label{tab:ppo_behavior}
    \begin{tabular}{lc}
        \toprule
        \textbf{Metric} & \textbf{Value} \\
        \midrule
        Mean trade time & 12.2 $\pm$ 14.0 steps \\
        Early trades (t $<$ 30) & 91.4\% \\
        Mid trades (30 $\leq$ t $<$ 70) & 6.9\% \\
        Late trades (t $\geq$ 70) & 1.7\% \\
        \midrule
        Shade actions & 60.6\% \\
        Truthful actions & 16.0\% \\
        Pass actions & 11.3\% \\
        Accept/Trade actions & 2.3\% \\
        Mean shade percentage & 24.4\% $\pm$ 16.5\% \\
        \bottomrule
    \end{tabular}
\end{table}

PPO exhibits aggressive early trading behavior: the vast majority of trades (91.4\%) occur in the first third of the period, with mean trade time of only 12.2 steps. The agent frequently chooses to shade its bids (60.6\% of actions), bidding below its private valuation to capture profit margin, while also making truthful bids (16.0\%) when conditions favor immediate execution.

The shade distribution reveals a bimodal strategy. Table \ref{tab:ppo_shade} shows the distribution of bid shading when PPO chooses to shade.

\begin{table}[H]
    \centering
    \caption{PPO Shade Distribution (v10 8M checkpoint)}
    \label{tab:ppo_shade}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Shade Range} & \textbf{Count} & \textbf{Percentage} \\
        \midrule
        0--5\% & 284 & 18.7\% \\
        5--10\% & 169 & 11.2\% \\
        10--20\% & 268 & 17.7\% \\
        20--30\% & 41 & 2.7\% \\
        30--40\% & 13 & 0.9\% \\
        40--50\% & 738 & 48.7\% \\
        \bottomrule
    \end{tabular}
\end{table}

Two modes dominate: 48.7\% of bids shade 40--50\% below valuation, while 18.7\% shade only 0--5\%. This suggests PPO learned a conditional strategy: bid conservatively (40--50\% shade) to maximize potential margin, but switch to aggressive near-valuation bids (0--5\% shade) when market conditions favor immediate execution.

\subsubsection{PPO vs Kaplan: Quantitative Behavioral Comparison}

To rigorously evaluate PPO's relationship to the winning Santa Fe strategy, we conduct a direct behavioral comparison using the same experimental setup (5 seeds, 5 periods, ZIC opponents) for both Kaplan and PPO agents. Table \ref{tab:ppo_kaplan_comparison} presents the results.

\begin{table}[H]
    \centering
    \caption{Behavioral Comparison: PPO vs Kaplan (5 seeds, 5 periods each, v10 8M checkpoint)}
    \label{tab:ppo_kaplan_comparison}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{Kaplan} & \textbf{PPO} & \textbf{Interpretation} \\
        \midrule
        Dominant action & PASS (68.9\%) & Shade (60.6\%) & Kaplan waits, PPO bids \\
        Mean trade time & 51.1 steps & 12.2 steps & 4x later \\
        Early trades (t $<$ 30) & 13.7\% & 91.4\% & Opposite timing \\
        Mid trades (30--70) & 62.7\% & 6.9\% & Kaplan zone \\
        Late trades (t $\geq$ 70) & 23.5\% & 1.7\% & Mostly Kaplan \\
        Total profit & 2,761 & 2,354 & Kaplan +17\% \\
        Profit per trade & 54.1 & 40.6 & Kaplan +33\% \\
        \bottomrule
    \end{tabular}
\end{table}

The comparison reveals that PPO and Kaplan employ fundamentally different temporal strategies. Kaplan implements patient waiting, passing on 69\% of opportunities and concentrating trades in the mid-to-late period (63\% in steps 30--70, 24\% after step 70). PPO implements early aggression, completing 91\% of trades before step 30. The strategies are temporal opposites.

However, both strategies share a critical commonality: conservative bid shading. Both Kaplan and PPO shade their bids below private valuations rather than bidding at value. This shared characteristic explains their mutual dominance over naive strategies. The difference lies in when each strategy chooses to execute: Kaplan waits for other traders to narrow the spread (parasitic sniping), while PPO aggressively captures early trades before competition develops (preemptive sniping).

This finding refines the narrative. PPO did not rediscover the Kaplan strategy. Instead, deep reinforcement learning discovered an alternative path to profitability: where Kaplan succeeds through patience, PPO succeeds through speed. Both exploit conservative margins, but through opposite temporal mechanisms. The emergence of this distinct strategy through pure trial-and-error learning demonstrates that multiple equilibria exist in double auction markets, and that RL can discover novel solutions that hand-crafted heuristics missed.

\subsubsection{Behavioral Adaptation to Opponent Sophistication}

A critical question arises: does PPO's learned strategy depend on facing naive opponents, or is it robust to sophisticated competition? We compare PPO's behavior when facing ZIC opponents versus a mixed population of Skeleton, ZIP, and Kaplan traders.

\begin{table}[H]
    \centering
    \caption{PPO Behavior: ZIC vs Mixed Opponents (5 seeds, 5 periods each, v10 8M checkpoint)}
    \label{tab:ppo_behavior_comparison}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{vs ZIC} & \textbf{vs MIXED} & \textbf{Change} \\
        \midrule
        Shade actions & 60.6\% & 60.0\% & $-$0.6pp \\
        Early trades (t $<$ 30) & 91.4\% & 94.9\% & +3.5pp \\
        Mean trade time & 12.2 steps & 10.0 steps & $-$2.2 \\
        Mean shade percentage & 24.4\% & 28.5\% & +4.1pp \\
        \bottomrule
    \end{tabular}
\end{table}

Table \ref{tab:ppo_behavior_comparison} reveals that PPO intensifies its trading strategy against sophisticated opponents. Against the mixed population, PPO trades faster (mean time 10.0 vs 12.2 steps), executes even more trades early (94.9\% vs 91.4\%), and bids more conservatively (28.5\% vs 24.4\% mean shade).

\begin{table}[H]
    \centering
    \caption{PPO Profit Analysis by Opponent Type (5 seeds, 5 periods each)}
    \label{tab:ppo_profit_comparison}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Metric} & \textbf{vs ZIC} & \textbf{vs MIXED} & \textbf{Change} \\
        \midrule
        Total Profit & 2,354 & 2,467 & +5\% \\
        Profit per Trade & 39.9 & 42.5 & +7\% \\
        \bottomrule
    \end{tabular}
\end{table}

This behavioral intensification produces superior outcomes. Table \ref{tab:ppo_profit_comparison} shows that PPO earns 5\% higher total profit and 7\% higher profit per trade against sophisticated opponents compared to naive ZIC traders. The learned strategy is not merely adequate against competition but becomes more effective when facing skilled counterparties.

This finding carries important implications for the robustness of deep RL in competitive markets. Rather than being exploited by sophisticated opponents, PPO adapts by intensifying its core tactics: earlier execution and more conservative pricing. The preemptive sniping strategy is not a fragile artifact of training against naive opponents but a genuinely robust approach that performs even better under competitive pressure.

\subsection{Summary}

The PPO experiments establish three key findings. First, deep reinforcement learning can discover trading strategies that outperform three decades of hand-crafted heuristics in double auction markets. PPO achieves rank 1 in the extended tournament, surpassing Ringuette, EL, Kaplan, and all other legacy strategies.

Second, PPO exhibits environment-dependent performance. In favorable-spread environments (BASE, BBBS, BSSS, PER, TOK), PPO maintains high efficiency and positive invasibility. In challenging environments with compressed margins (EQL, SML), PPO underperforms ZIC, suggesting the learned policy is specialized for environments with sufficient price spread.

Third, the learned policy adopts a distinctive temporal strategy, executing nearly all trades in the opening third of each period. This early aggression contrasts with the patient strategies of legacy algorithms and may explain both PPO's success against slow-adapting opponents and its vulnerability in compressed-margin environments where early aggressive pricing is unprofitable.
