\subsection{Single-Agent Invasion}
We first evaluate the ability of the PPO agent to exploit a population of legacy heuristic agents. We train a single PPO agent against Skeleton agents for $10^6$ time steps using the standard market configuration (4 buyers, 4 sellers, 100 steps per period). The PPO agent uses a MaskablePPO architecture with action masking to ensure valid bids under AURORA protocol constraints.

\subsection{Mixed Market Tournament}

We evaluate PPO performance in a mixed market tournament against legacy strategies (Skeleton, GD, ZIP, ZIC, Kaplan). A critical finding emerged during evaluation: the PPO model trained as a buyer exhibited significantly different performance when deployed in both buyer and seller roles versus buyer-only deployment.

When restricted to the buyer role (matching its training distribution), PPO achieves first place in the tournament, outperforming all legacy strategies including the historically dominant Skeleton algorithm.

\begin{table}[H]
    \centering
    \caption{Mixed Market Tournament Results (PPO Buyer-Only)}
    \label{tab:ppo_tournament}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Strategy} & \textbf{Mean Profit} & \textbf{Rank} \\
        \midrule
        \textbf{PPO (Buyer)} & \textbf{1204.7} & \textbf{1} \\
        Skeleton & 1196.5 & 2 \\
        GD & 1173.4 & 3 \\
        ZIP & 1172.8 & 4 \\
        ZIC & 880.3 & 5 \\
        Kaplan & 814.1 & 6 \\
        \bottomrule
    \end{tabular}
\end{table}

The PPO agent outperforms the previous tournament champion (Skeleton) by 0.7\%, demonstrating that reinforcement learning can discover strategies that surpass decades of hand-crafted heuristics. The performance gap is modest but statistically significant over 50 rounds and 10 periods each.

\subsection{Role Specialization}

An important methodological finding concerns role specialization. When the buyer-trained PPO model was deployed in both buyer and seller roles (the initial experimental setup), it ranked third behind Skeleton and GD. Investigation revealed that the model had never observed the seller perspective during training, leading to poor seller-side decision making. This suggests that double auction agents may benefit from role-specific training rather than universal models.

We are currently training a seller-specific PPO model to enable full tournament evaluation with separate buyer and seller specialists.

\subsection{The Neural Market Equilibrium}
In the second set of experiments, we replace all agents with PPO learners (Self-Play). Table \ref{tab:efficiency_matrix} summarizes the allocative efficiency of the resulting markets compared to the baseline Kaplan and ZI-C markets.

\begin{table}[H]
    \centering
    \caption{Allocative Efficiency Matrix (Mean $\pm$ Std. Dev)}
    \label{tab:efficiency_matrix}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Market Composition} & \textbf{vs. ZI-C} & \textbf{vs. Kaplan} & \textbf{vs. Self-Play} \\
        \midrule
        \textbf{Kaplan (Baseline)} & $99.1\% \pm 0.2$ & $98.5\% \pm 0.5$ & $55.0\% \pm 12.0$ \\
        \textbf{PPO (Ours)} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        \bottomrule
    \end{tabular}
\end{table}
