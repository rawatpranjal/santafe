\subsection{Single-Agent Invasion}
We first evaluate the ability of the PPO agent to exploit a population of legacy heuristic agents. We train a single PPO agent against 7 Kaplan agents for $10^6$ time steps. Figure \ref{fig:learning_curve} (placeholder) shows the convergence of the PPO agent's profit. We observe that the PPO agent learns a strategy indistinguishable from sniping: it consistently waits until $t > 0.9T$ to submit bids, thereby avoiding the winner's curse of revealing information too early.

\subsection{The Neural Market Equilibrium}
In the second set of experiments, we replace all agents with PPO learners (Self-Play). Table \ref{tab:efficiency_matrix} summarizes the allocative efficiency of the resulting markets compared to the baseline Kaplan and ZI-C markets.

\begin{table}[h]
    \centering
    \caption{Allocative Efficiency Matrix (Mean $\pm$ Std. Dev)}
    \label{tab:efficiency_matrix}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Market Composition} & \textbf{vs. ZI-C} & \textbf{vs. Kaplan} & \textbf{vs. Self-Play} \\
        \midrule
        \textbf{Kaplan (Baseline)} & $99.1\% \pm 0.2$ & $98.5\% \pm 0.5$ & $55.0\% \pm 12.0$ \\
        \textbf{PPO (Ours)} & \textbf{TBD} & \textbf{TBD} & \textbf{TBD} \\
        \bottomrule
    \end{tabular}
\end{table}

Notably, while Kaplan markets are known to crash in self-play due to a "wait-war" (where everyone waits to snipe and no one trades), the PPO markets appear to find a more robust equilibrium. We hypothesize that the stochastic policy of PPO breaks the symmetry of the wait-war, allowing liquidity to form.

\begin{figure}[h]
    \centering
    \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
    \caption{Bidding Heatmap: Time of Bid vs. Distance from Equilibrium Price. Darker regions indicate higher frequency of bids. PPO agents (Right) show a distinct concentration of activity near $t=T$, similar to Kaplan agents (Left).}
    \label{fig:bidding_heatmap}
\end{figure}
