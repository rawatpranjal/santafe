This section describes all trading algorithms evaluated in this study, organized by complexity: zero-intelligence baselines, adaptive heuristics from the 1993 Santa Fe Tournament, and modern AI agents (reinforcement learning and large language models).

\subsection{Zero-Intelligence Algorithms}

\subsubsection{Zero Intelligence (ZI)}
Zero Intelligence (ZI) represents the simplest possible trading strategy, serving as a control condition from Gode and Sunder (1993). The agent generates bids and asks by drawing uniformly from the price range: $p \sim U[p_{min}, p_{max}]$ where $p_{min} = 1$ and $p_{max} = 1000$ in our implementation. Critically, ZI agents have no budget constraint and will accept any trade if selected as winner, regardless of profitability. This unconstrained randomness provides a baseline for measuring the contribution of strategic behavior to market efficiency. The algorithm contains no learning parameters and maintains no memory of previous trades.

\subsubsection{Zero Intelligence Constrained (ZIC)}
Zero Intelligence Constrained (ZIC) extends ZI by adding budget constraints that prevent unprofitable trades. For buyers with valuation $V$ for the current token, bids are generated as $b = V - \lfloor U[0,1) \times (V - p_{min}) \rfloor$ using floor truncation matching the original Java implementation. For sellers with cost $C$, asks follow $a = C + \lfloor U[0,1) \times (p_{max} - C) \rfloor$. The agent accepts trades only when profitable: buyers accept if $V > a_{ask}$ and sellers accept if $b_{bid} > C$, using strict inequalities that differ slightly from the theoretical formulation but match the 1993 baseline. This simple constraint dramatically improves efficiency without requiring any learning or strategic reasoning.

\subsubsection{Zero Intelligence Plus (ZIP)}
Zero Intelligence Plus (ZIP) adapts profit margins using the Widrow-Hoff delta rule from machine learning \citep{cliff1997}. The agent shouts at price $p = \lambda(1 + \mu)$ where $\lambda$ is the limit price (valuation for buyers, cost for sellers) and $\mu$ is the profit margin (negative for buyers, positive for sellers). The margin updates according to $\Delta(t) = \beta(\tau(t) - p(t))$ where $\tau$ is a target price, followed by momentum accumulation $\Gamma(t+1) = \gamma \Gamma(t) + (1-\gamma)\Delta(t)$. The target uses random perturbations based on recent market activity: $\tau = R \cdot q + A$ with $R \sim U[R_{min}, R_{max}]$ and $A \sim U[A_{min}, A_{max}]$ where $q$ is the last relevant shout price. Key parameters calibrated for AURORA markets: $\beta = 0.2$ (learning rate), $\gamma = 0.25$ (momentum coefficient), initial margin $\mu_0 = \pm 0.20$, with $R$ perturbations in $[0.95, 1.05]$ and $A$ perturbations in $[-0.05, 0.05]$. The algorithm responds to both accepted trades by raising margins when own price was far from transaction price and rejected orders by lowering margins when not competitive with market quotes.

\subsubsection{Zero Intelligence Two (ZI2)}
Zero Intelligence Two (ZI2) enhances ZIC by incorporating the current market bid and ask into the random price generation. For buyers facing current bid $b_{curr}$, if $b_{curr} > 0$ and $b_{curr} \leq V$, the agent narrows the random range to $b = V - \lfloor U[0,1) \times (V - b_{curr}) \rfloor$, effectively randomizing only above the standing bid. Sellers apply symmetric logic with current ask $a_{curr}$. When the current quote exceeds the agent's valuation (buyers) or falls below cost (sellers), the algorithm generates extreme quotes ($p_{min}$ or $p_{max}$) to signal inability to compete. This market-awareness allows ZI2 to adapt to trading activity without learning, though it retains zero intelligence in the sense of having no predictive model or memory across periods.

\subsection{Santa Fe Tournament Algorithms}

\subsubsection{Gjerstad-Dickhaut (GD)}
Gjerstad-Dickhaut (GD) forms probabilistic beliefs from historical data and maximizes expected surplus \citep{gjerstad1998}. For sellers choosing ask $a$, the probability of acceptance is $p(a) = [T_A(\geq a) + B(\geq a)] / [T_A(\geq a) + B(\geq a) + R_A(\leq a)]$ where $T_A(\geq a)$ counts accepted asks at or above $a$, $B(\geq a)$ counts all bids at or above $a$, and $R_A(\leq a)$ counts rejected asks at or below $a$. Buyers use the symmetric formulation $q(b) = [T_B(\leq b) + A(\leq b)] / [T_B(\leq b) + A(\leq b) + R_B(> b)]$ for bid $b$. The agent then maximizes expected surplus: sellers choose $a^* = \arg\max_{a \in [C, p_{max}]} p(a) \times (a - C)$ and buyers choose $b^* = \arg\max_{b \in [p_{min}, V]} q(b) \times (V - b)$ where $C$ is cost and $V$ is valuation. Implementation uses PCHIP (monotone cubic spline) interpolation to smooth the belief functions and maintains a memory of the last $L = 100$ trades. The agent accepts immediate trades only when certain surplus exceeds expected surplus from optimal quote.

\subsubsection{Kaplan}
Kaplan implements a strategic sniper strategy that waits for favorable conditions before jumping into the market \citep{rust1994}. The algorithm tracks price history across periods, computing $\bar{p}$, $p_{min}$, and $p_{max}$ separately for each role. In the first bid or ask of each period, the agent uses the worst-case token value adjusted by market conditions. Subsequent quotes employ jump-in logic triggered by three conditions: small spread ($\langle a_{curr} - b_{curr} \rangle / a_{curr} < 0.10$ for buyers), price better than last period ($a_{curr} \leq p_{min}$ for buyers or $b_{curr} \geq p_{max}$ for sellers), or time pressure measured as $(t - t_{last}) \geq (T - t)/2$ where $T$ is the period length. When jump-in triggers, buyers bid at the current ask and sellers ask at the current bid, though protection clauses prevent losses: $b_{new} \leq V - 1$ for buyers and $a_{new} \geq C + 1$ for sellers. In the buy-sell phase, the agent becomes a sniper in the final two timesteps, accepting any profitable trade.

\subsubsection{Lin}
Lin employs statistical price prediction using normal distribution sampling via the Box-Muller transform. The algorithm computes mean price $\bar{p} = \sum |prices| / n$ and standard error $\sigma = \sqrt{\sum(|p| - \bar{p})^2 / (n-1)}$ from current period data, then extends this to a target price $\tau$ incorporating all previous periods: $\tau = (\bar{p}_{current} + \sum_{i=1}^{period-1} \bar{p}_i) / period$. To generate bids, the agent samples from a normal distribution $\mathcal{N}(\bar{p}, \sigma)$ using Box-Muller and combines this with a weighted average of conservative and target prices: $b_{new} = w \cdot (b_{curr}+1) + (1-w) \cdot \tau$ where the weight $w$ incorporates time pressure, inventory position, and market composition. The buy-sell decision uses threshold acceptance: buyers accept if $a_{curr} < \tau + \sigma$ and sellers accept if $b_{curr} > \tau - \sigma$. This statistical approach attempts to predict equilibrium prices from historical data without explicit belief formation.

\subsubsection{Jacobson}
Jacobson computes a weighted equilibrium estimate that gains confidence as trading progresses. On each trade, the algorithm updates $\tau_{eq} = \sum_{trades} (price \times weight) / \sum_{trades} weight$ where weight increases with both period number and trade count: $weight = period + n_{trades} \times \alpha$ with $\alpha = 2.0$. Confidence in this estimate follows an exponential function $conf = \beta^{1/\sum weight}$ where $\beta = 0.01$, approaching unity as total weight accumulates. Bids are generated as $b_{new} = b_{old} \cdot (1-conf) + \tau_{eq} \cdot conf + \delta$ where $\delta = 1.0$ is a bid-ask offset and $b_{old}$ is the current standing bid (or $p_{min}$ if none). Asks follow symmetric logic with negative offset. The buy-sell decision employs complex gap analysis: if spread $gap = a - b$ equals the previous gap or time pressure condition $(gap/(gap_{last} - gap)) \times n_{tokens} \times 2.0 + t > T$ holds, accept probabilistically with $prob = profit/(profit + gap)$. The four tunable hyperparameters allow adaptation to different market microstructures.

\subsubsection{Perry}
Perry implements adaptive learning with efficiency-based parameter self-tuning across periods. The core adaptive parameter $a_1$ scales with time pressure, market composition, and role imbalance: for buyers, $a_1 = a_0 \times (T-t)/T \times (N-1)/N \times n_{sellers}/n_{buyers}$ where $a_0$ begins at $2.0$ and adjusts based on period performance. After the first three conservative trades in each period, the algorithm uses statistical bidding: buyers bid $b = \bar{p} + 0.2\sigma - a_1\sigma + U[0,1] \times 4s$ where $\bar{p}$ is mean price, $\sigma$ is standard deviation, and $s \in \{-1,+1\}$ is random, while sellers ask $a = \bar{p} + a_1\sigma + 20 \cdot U[0,1]$. At period end, Perry evaluates efficiency $e = profit_{actual} / profit_{potential}$ where potential profit sums over all feasible tokens. If $e < 1.0$, the algorithm tunes itself: when $e = 0$ it sets $a_0 \leftarrow a_0/3$, otherwise $a_0 \leftarrow a_0 \times e$. This self-tuning allows Perry to adapt to changing market conditions across the session without external calibration.

\subsubsection{Skeleton}
Skeleton provides a simplified template strategy combining elements of Kaplan's logic with random weighting. The algorithm generates parameter $\alpha = 0.25 + 0.1 \times U[0,1]$ each time it quotes. For first bids, it computes conservative bound $most = V_{worst} - 1$ adjusted by current ask if better, then bids $b = most - \alpha \times (V_{best} - V_{worst})$ where the spread term captures token value range. Subsequent bids use weighted average: $b_{new} = (1-\alpha)(b_{curr}+1) + \alpha \cdot most$ interpolating between improving current bid and maximum willing to pay. Asks follow symmetric logic. In the buy-sell phase, the agent computes target price as $\tau = 1.3 V_{worst} - 0.3 V_{best}$ and interpolates with current token value using time-based weight $\alpha = 1.0/(t - t_{last})$. This simple structure serves as a baseline demonstrating basic strategic concepts without sophisticated learning or prediction mechanisms.

\subsection{Modern AI Agents}

\subsubsection{The Gradient Trader (PPO)}
To test whether modern reinforcement learning can rediscover or surpass hand-crafted heuristics, we deploy agents trained with Proximal Policy Optimization (PPO) \citep{schulman2017}. Unlike the heuristic agents of 1993, the PPO agent has no hard-coded rules. It perceives the market through a normalized observation vector $O_t \in \mathbb{R}^{12}$:
\begin{equation}
    O_t = [v_{i}, \text{inventory}_i, t/T, b^*_t, a^*_t, s_t, p_{last}, \dots]
\end{equation}
The agent outputs a discrete action $u_t \in \{ \text{Pass}, \text{Accept}, \text{Improve}, \text{Match} \}$. The reward function is simply the realized profit from trade: $r_t = v_i - p_t$ for buyers and $r_t = p_t - c_i$ for sellers.

The architecture uses an Actor-Critic framework with an LSTM layer to capture temporal dependencies. The observation vector is processed by a dense feature extraction layer (64 units), fed into an LSTM layer (64 units), then branches into separate Actor (policy) and Critic (value) heads. Training employs curriculum learning: the agent initially trains against ZIC traders, then faces progressively more sophisticated opponents (ZIP, Kaplan) as proficiency increases. Full architectural details appear in Appendix~\ref{app:ppo_trader}.

\subsubsection{The Semantic Trader (LLM)}
To evaluate whether pre-trained language models can compete with specialized trading algorithms, we introduce agents driven by Large Language Models (GPT-4o, GPT-4o-mini). The ``Semantic Trader'' receives a textual representation of the market state and outputs structured JSON trading decisions. Unlike PPO agents that require extensive training, LLMs operate zero-shot, leveraging pre-trained knowledge to interpret market conditions.

The prompt structure uses condition-action framing with concrete examples to minimize hallucination:
\begin{quote}
    \textit{``You are a trader in a double auction. Your goal is to maximize profit. BUYERS: You profit when you BUY BELOW your valuation. Your bid must be LESS THAN your valuation AND HIGHER than current best bid. Example: If valuation=200, current bid=150, you can bid 151-199.''}
\end{quote}

We parse the structured JSON output to execute trades. Systematic prompt engineering revealed that concrete examples outperform abstract strategic guidance, and that verbose market knowledge actually degrades performance. Full prompt specifications appear in Appendix~\ref{app:llm_trader}.
