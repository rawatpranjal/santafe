\section{Introduction}

How do decentralized markets coordinate the actions of self-interested agents without central authority? This question, first articulated by \citet{hayek1945} as the "knowledge problem," remains one of the most profound puzzles in economics. Hayek argued that no central planner could ever possess the dispersed, tacit knowledge held by millions of individual participants, yet markets routinely aggregate this information into prices that guide efficient resource allocation. The mechanism by which this coordination occurs, without explicit communication or shared intent, has been the subject of decades of experimental and computational inquiry.

The experimental investigation of this phenomenon began with Vernon Smith's pioneering work in the 1960s, which demonstrated that simple market institutions, particularly the Double Auction, could reliably converge to competitive equilibrium even when participants possessed only private information about their own costs and valuations. This finding launched a research program that culminated in the 1990 Santa Fe Double Auction Tournament, where computer programs competed to maximize profits in an artificial market. The tournament produced a striking and paradoxical result: the winning strategy was not a sophisticated learning algorithm but a simple "sniping" heuristic that exploited the information revealed by other traders. Yet this individually optimal strategy proved collectively destructive; markets composed entirely of snipers collapsed into illiquidity. The tension between individual rationality and collective efficiency, between structure and agency, between intelligence and stability, remains unresolved.

The rise of artificial intelligence has made these questions urgently relevant. Modern financial markets are increasingly dominated by algorithmic traders, and regulators have raised concerns about the potential for autonomous agents to discover tacit collusion or destabilize market function. At the same time, large language models have demonstrated surprising capabilities in reasoning and planning, raising the question of whether semantic knowledge alone can support effective economic behavior. This study revisits the foundational questions of the Santa Fe Tournament through the lens of modern AI, specifically Deep Reinforcement Learning and Large Language Models, to investigate whether these "super-rational" and "semantic" agents can solve the market coordination problem without hand-crafted heuristics, and what their success or failure reveals about the fundamental nature of price discovery.

The literature on market microstructure has generated several enduring debates that frame our investigation. First, there is the question of whether market efficiency is a property of the institution or the participants: the "Zero-Intelligence" experiments of Gode and Sunder suggested that even random traders can achieve near-perfect allocative efficiency in a Double Auction, implying that the market structure itself does most of the computational work. Yet subsequent research revealed that while structure ensures efficiency, intelligence determines equity; zero-intelligence traders exhibited massive profit dispersion compared to human markets, suggesting that strategic sophistication is required for agents to secure their "fair share" of the gains from trade. Second, there is the evolutionary dynamics of trading strategies: the Santa Fe Tournament and subsequent genetic programming experiments demonstrated that dominant strategies are not stable equilibria but nodes in an endless arms race, where each successful heuristic creates selection pressure for counter-strategies. Finally, there is the question of liquidity provision: the optimal individual strategy of waiting and sniping is parasitic, requiring other agents to reveal information and absorb adverse selection, yet a market of pure snipers collapses. These debates, reviewed in Section 2, provide the theoretical scaffolding for our empirical investigation.

\subsection{Motivation and Broader Relevance}

The motivation for this research is threefold. First, the "Hayek Hypothesis" that markets efficiently aggregate private information has traditionally been tested using human subjects or simple heuristic agents. It remains an open question whether the convergence properties observed in these studies are robust to agents with vastly superior computational capabilities (DRL) or broad semantic knowledge (LLMs). If advanced AI agents can disrupt market stability or uncover novel forms of algorithmic collusion, the theoretical underpinnings of market efficiency may need to be re-evaluated.

Second, the rise of automated trading has transformed financial markets from human-dominated ecosystems into arenas of algorithmic competition. However, most academic studies of algorithmic trading rely on proprietary data or complex, high-fidelity simulations that obscure the fundamental economic dynamics. By returning to the stylized, scientifically controlled environment of the Santa Fe Double Auction, we can isolate the effects of agent intelligence from market microstructure noise, providing clearer insights into the nature of algorithmic competition.

Third, there is a theoretical disconnect between the "Zero-Intelligence" view, which attributes efficiency solely to market structure, and the game-theoretic view, which requires sophisticated belief modeling. Modern AI offers a unique tool to probe this divide: DRL agents learn strategies from scratch without human priors, while LLMs bring a form of "common sense" reasoning to trading. Observing how these distinct forms of intelligence navigate the trade-off between liquidity provision and surplus extraction will deepen our understanding of price formation.

\subsection{Research Questions}

This study is guided by four primary research questions, designed to test the limits of both the market institution and the artificial agents.

The first question concerns whether Deep Reinforcement Learning can rediscover and outperform the dominant heuristics of the Santa Fe Tournament. The "Kaplan" strategy, a simple sniping heuristic, dominated the original 1990 tournament. We investigate whether a Proximal Policy Optimization (PPO) agent, starting with no prior knowledge of market rules or opponent strategies, can learn a policy that exploits Kaplan and other legacy algorithms. This probes whether the "sniping" behavior is a fundamental attractor of the strategy space or merely a local optimum of heuristic design.

The second question asks whether a market composed entirely of autonomous AI agents can remain stable. Previous work has shown that markets populated exclusively by sniping agents collapse due to a lack of liquidity. We examine whether a population of independent PPO agents, trained via self-play, can avoid this "liquidity trap" and converge to a competitive equilibrium. Specifically, does the gradient-based learning process discover a mixed strategy of liquidity provision and taking that sustains market function, or does it devolve into algorithmic collusion?

Third, we investigate whether Large Language Models can trade effectively in a zero-shot setting. LLMs possess vast semantic knowledge but lack the specific iterative training of RL agents. We test whether a general-purpose model such as GPT-4o, provided only with a textual description of the market state and history, can execute profitable trading strategies. This addresses the "semantic hypothesis": that understanding the \textit{context} of a market is sufficient for rational behavior, even without explicit optimization.

Finally, we ask how intelligence disparity affects wealth distribution. In a heterogeneous market populated by agents of varying cognitive capacities, we examine the extent of wealth transfer from less capable to more capable agents. This quantifies the "value of intelligence" in a double auction and provides a proxy for the potential impact of AI disparity in real-world financial markets.

\subsection{Hypotheses and Expected Outcomes}

We formulate specific hypotheses corresponding to our research questions, grounded in the prior literature.

Regarding PPO behavior against legacy agents, we hypothesize that a single PPO agent trained against a diverse pool of legacy agents (ZI-C, Kaplan, ZIP) will converge to a "sniping" strategy, characterized by withholding bids until the final moments of a trading period. We expect PPO to rediscover the optimal procrastination strategy identified by \citet{chen2011}, likely executing it with greater precision than the static Kaplan heuristic. Consequently, we anticipate the PPO agent will achieve higher profits than any individual legacy opponent.

Concerning market stability under AI-only conditions, we hypothesize that a market composed entirely of PPO agents will maintain high allocative efficiency (greater than 95 percent), avoiding the market collapse observed in Kaplan-only markets. Unlike static heuristics, RL agents are capable of adapting to the aggregate state of the market. We expect that in self-play, PPO agents will learn to provide just enough liquidity to ensure trades occur, thereby avoiding the zero-volume outcome of the "waiting game" equilibrium described by \citet{wilson1987}. However, we also anticipate a secondary effect: PPO agents may learn to maintain wider bid-ask spreads than human traders, exhibiting a form of tacit algorithmic collusion.

With respect to LLM performance, we hypothesize that zero-shot LLM agents will achieve allocative efficiency comparable to human subjects but will underperform optimized RL agents. We expect LLMs to avoid the chaotic behavior of unconstrained zero-intelligence traders, demonstrating a baseline of economic rationality derived from their training data. However, without the specific feedback loops of reinforcement learning, they are unlikely to master the precise timing and order-book pressure tactics required to beat a trained PPO sniper.

Finally, regarding wealth distribution under intelligence disparity, we hypothesize that in a mixed market of GPT-4o and GPT-3.5 agents, the superior model will extract a disproportionate share of the surplus, with the wealth gap exceeding the difference in their allocative efficiency contributions. This posits that "smarter" agents do not necessarily make the market more efficient; rather, they are more effective at rent-seeking. We expect GPT-4o to better identify and exploit the sub-optimal bids of GPT-3.5, resulting in a significant transfer of producer and consumer surplus.

\subsection{Contributions}

This work makes three distinct contributions to the literature on agent-based computational economics. First, it provides the first direct comparison of Deep Reinforcement Learning and Large Language Models within the rigorous, scientifically controlled environment of the Santa Fe Double Auction. By benchmarking these modern AI paradigms against the canonical "Legacy Zoo" of trading heuristics (ZI-C, Kaplan, ZIP, GD), we establish a clear continuity between the experimental economics of the 1990s and the AI research of the 2020s.

Second, we offer a methodological contribution by creating a high-fidelity, open-source Python implementation of the Santa Fe tournament platform, integrated with modern MLOps standards (Gymnasium, Stable-Baselines3). This "modernized testbed" lowers the barrier to entry for future research into AI market behavior, replacing the inaccessible or deprecated codebases of previous decades.

Finally, our analysis of the "implicit markup" and "belief functions" of PPO agents offers a novel interpretability framework for neural trading agents. By mapping the opaque policy networks of DRL back onto the economic theory of markups \citep{zhan2007} and belief functions \citep{gjerstad1998}, we demystify the "black box" of AI trading, showing that these agents are not learning alien strategies, but rather rediscovering and refining the fundamental economic principles of price discovery.
